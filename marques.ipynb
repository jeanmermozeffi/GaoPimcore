{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Importation des Bibliothèques et Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import unidecode\n",
    "import uuid\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.largus.fr/Toutes-Marques.html\"\n",
    "port = 59795"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Définition des fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, folder_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(folder_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Image téléchargée avec succès: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"Échec du téléchargement de l'image depuis l'URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver():\n",
    "    chrome_option = Options()\n",
    "    headless = True\n",
    "    chrome_option.binary_location = '/Applications/Brave Browser.app/Contents/MacOS/Brave Browser'\n",
    "    \n",
    "    # if port:\n",
    "    #     chrome_options.add_argument(f'--remote-debugging-port={port}')\n",
    "    # if headless:\n",
    "    #     chrome_options.add_argument('--headless')\n",
    "\n",
    "    service = Service()\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_option)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_html(url, port=None, headless=False):\n",
    "    driver = get_driver()\n",
    "  \n",
    "    driver.get(url)\n",
    "\n",
    "    driver.implicitly_wait(30)\n",
    "\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Fermer le navigateur\n",
    "    # driver.quit()\n",
    "\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = get_page_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "marques = []\n",
    "\n",
    "for item in soup.find_all(\"div\", class_=\"liste-mm-item\"):\n",
    "    marque = {}\n",
    "    marque[\"libelle\"] = item.find(\"a\", class_=\"libelle\").text.strip()\n",
    "    marque[\"lien_url\"] = \"https://www.largus.fr\" + item.find(\"a\", class_=\"libelle\")[\"href\"].replace(\"\\\\/\", \"/\")\n",
    "    marque[\"logo_url\"] = \"https://www.largus.fr\" + item.find(\"img\")[\"src\"].replace(\"\\\\/\", \"/\")\n",
    "    marque[\"alt_text\"] = item.find(\"img\")[\"alt\"]\n",
    "    marques.append(marque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "marques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques = pd.DataFrame(marques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques.to_json(\"marques.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques.to_csv(\"marques.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des dossiers et téléchargement des logos\n",
    "for index, row in df_marques.iterrows():\n",
    "    libelle = row['libelle']\n",
    "    logo_url = row['logo_url']\n",
    "    \n",
    "    # Créer un dossier avec le libellé de la marque\n",
    "    folder_path = os.path.join(f\"Marque Folder/{libelle.capitalize()}\", 'Logo')\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Télécharger le logo dans le dossier 'logo'\n",
    "    logo_filename = os.path.basename(logo_url)\n",
    "    logo_path = os.path.join(folder_path, logo_filename)\n",
    "    download_image(logo_url, logo_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques = pd.read_json(\"marques.json\")\n",
    "df_marques = df_marques.sort_values(by='libelle')\n",
    "df_marques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_marques[df_marques['Traiter'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques_json = pd.read_json(\"object_mark_json.json\")\n",
    "df_marques_json = df_marques_json.sort_values(by='Name')\n",
    "df_marques_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques_json.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_marques.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vehicle_info(driver):\n",
    "    # Trouver tous les éléments de produit\n",
    "    try:\n",
    "        products_elements = driver.find_elements(By.CSS_SELECTOR, 'a.product-wrap')\n",
    "        if not products_elements:\n",
    "            print(\"No product elements found.\")\n",
    "            return []\n",
    "    except NoSuchElementException:\n",
    "        print(\"Error finding product elements.\")\n",
    "        return []\n",
    "\n",
    "    \n",
    "    vehicles = []\n",
    "\n",
    "    # Extraire les informations pour chaque véhicule\n",
    "    for element in products_elements:\n",
    "        try:\n",
    "            vehicle_url = element.get_attribute('href')\n",
    "            vehicle_model = element.get_attribute('data-model')\n",
    "            vehicle_make = element.get_attribute('data-make')\n",
    "            vehicle_title = element.find_element(By.CSS_SELECTOR, 'span.product-title').text\n",
    "\n",
    "            # Add extracted information to the list\n",
    "            vehicles.append({\n",
    "                'url': vehicle_url,\n",
    "                'model': vehicle_model,\n",
    "                'make': vehicle_make,\n",
    "                'title': vehicle_title\n",
    "            })\n",
    "        except NoSuchElementException as e:\n",
    "            print(f\"Error extracting data from element: {e}\")\n",
    "            continue\n",
    "\n",
    "    return vehicles\n",
    "\n",
    "\n",
    "def save_vehicles_to_csv(vehicles):\n",
    "    if vehicles:\n",
    "        folder = \"Modeles\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            print(f\"Folder created at {folder}\")\n",
    "\n",
    "        file_name = f\"{vehicles[0]['make']}.csv\"\n",
    "        save_path = os.path.join(folder, file_name)\n",
    "        pd.DataFrame(vehicles).to_csv(save_path, index=False)\n",
    "        print(f\"Data saved to {save_path}\")\n",
    "\n",
    "\n",
    "def scrape_multiple_urls(driver, url):\n",
    "    print(f\"Scraping URL: {url}\")\n",
    "    driver.get(url)\n",
    "\n",
    "    vehicles_info = extract_vehicle_info(driver)\n",
    "\n",
    "    if vehicles_info:\n",
    "        save_vehicles_to_csv(vehicles_info)\n",
    "\n",
    "def process_links(driver, dataframe):\n",
    "    filtered_df = df_marques[df_marques['Traiter'] == 1]\n",
    "    treated_links = set(filtered_df['lien_url'])  # Un ensemble pour stocker les liens déjà traités\n",
    "\n",
    "    # Vérifier si la colonne Traiter existe déjà\n",
    "    if 'Traiter' not in dataframe.columns:\n",
    "        dataframe['Traiter'] = 0\n",
    "\n",
    "    counter = 0\n",
    "    for index, row in dataframe.iterrows():\n",
    "        link_url = row['lien_url']\n",
    "        # Vérifier si le lien a déjà été traité\n",
    "        \n",
    "        if link_url in treated_links:\n",
    "            continue\n",
    "            \n",
    "        if link_url not in treated_links:\n",
    "            scrape_multiple_urls(driver, link_url)\n",
    "            dataframe.at[index, 'Traiter'] = 1\n",
    "            treated_links.add(link_url)\n",
    "            dataframe.to_json(\"marques.json\", orient=\"records\")\n",
    "            counter += 1\n",
    "\n",
    "            print(f\"Waiting for 1 minute before the next URL...{counter}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        if counter >= 50:\n",
    "            print(\"Arrêt après 50 itérations.\")\n",
    "            break\n",
    "    print(f\"Arrêt toutes les liens, un total de {counter} ont été traitées !.\")\n",
    "    #return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_central = 'https://www.largus.fr/Bmw.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver()\n",
    "process_links(driver, df_marques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Concatener le dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_column_path = 'Vehiculs/Models'\n",
    "folder_path = 'Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques_json['Name'] = df_marques_json['Name'].str.lower()\n",
    "final_df['make'] = final_df['make'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner les DataFrames sur les colonnes 'make' et 'Name'\n",
    "merged_df = pd.merge(final_df, df_marques_json[['id', 'Name']], left_on='make', right_on='Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dropna(subset=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_finite_values = merged_df['id'][~merged_df['id'].apply(np.isfinite)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(non_finite_values) == 0:\n",
    "    merged_df['id'] = merged_df['id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['id'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['folder_column_path'] = merged_df['make'].apply(lambda make: f\"Vehiculs/Models/{make.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_cleaned = merged_df[['url', 'model', 'make', 'title', 'id', 'folder_column_path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marques_json.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer le DataFrame final dans un nouveau fichier CSV\n",
    "final_df_cleaned.to_csv('Modeles/model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_cleaned = pd.read_csv('Modeles/model.csv')\n",
    "final_df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver()\n",
    "driver.get(url_central)\n",
    "time.sleep(2)\n",
    "html_content = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les informations extraites\n",
    "models = extract_vehicle_info(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## les fiches techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fiche_technique = 'https://www.largus.fr/Audi_A3-Berline.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_captcha(soup):\n",
    "    iframe = soup.find('iframe')\n",
    "    if iframe != -1:\n",
    "        # Obtenir la valeur de l'attribut src de l'iframe\n",
    "        src = iframe.get('src')\n",
    "        # Vérifier si l'attribut src commence par le lien spécifique du captcha\n",
    "        if src.startswith('https://geo.captcha-delivery.com/captcha/?initialCid='):\n",
    "            return True\n",
    "    # Retourner False si aucun iframe n'est trouvé ou si l'attribut src ne commence pas par le lien spécifique\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver()\n",
    "driver.get(url_fiche_technique)\n",
    "time.sleep(2)\n",
    "html_content = driver.page_source\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "iframe = soup.find('iframe')\n",
    "iframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detect_captcha(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_make_from_url(url):\n",
    "    match = re.search(r'/fiche-technique/([^/]+)/', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def extract_year_from_libelle(libelle):\n",
    "    try:\n",
    "        match = re.search(r'\\b\\d{4}\\b', libelle)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting year from libelle: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Fonction pour extraire le lien \"Toutes les fiches techniques\"\n",
    "def extract_all_fiches_techniques_url(html_content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        section = soup.select_one('section.stacking-block.section-fiches-techniques')\n",
    "\n",
    "        if section:\n",
    "            lien_tout = section.select_one('a.lien-tout')\n",
    "            if lien_tout:\n",
    "                return lien_tout.get('href')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting 'lien-tout': {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour extraire les fiches techniques d'une page donnée\n",
    "def extract_fiches_techniques(driver, url, model):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        response = driver.page_source\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL: {url}. Exception: {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        fiches = []\n",
    "        marque = extract_make_from_url(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing HTML content from URL: {url}. Exception: {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        for item in soup.select('ul.liste-millesimes li a.item'):\n",
    "            try:\n",
    "                libelle = item.select_one('span.libelle').text.strip()\n",
    "                lien = item.get('href')\n",
    "                year = extract_year_from_libelle(libelle)\n",
    "                fiches.append({\n",
    "                    'Libelle': libelle,\n",
    "                    'Marque': marque,\n",
    "                    'Model': model,\n",
    "                    'Lien': f\"https://www.largus.fr{lien}\",\n",
    "                    'Annee': year\n",
    "                })\n",
    "            except AttributeError as e:\n",
    "                print(f\"Error extracting data from an item: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing items from URL: {url}. Exception: {e}\")\n",
    "        return []\n",
    "\n",
    "    return fiches\n",
    "\n",
    "\n",
    "def process_all_fiches_techniques(html_content_fiche, model, make):\n",
    "    all_urls = extract_all_fiches_techniques_url(html_content_fiche)\n",
    "        \n",
    "    if all_urls:\n",
    "        print(f\"Lien vers toutes les fiches techniques: {all_urls}\")\n",
    "\n",
    "        # Compléter l'URL si nécessaire\n",
    "        if not all_urls.startswith('http'):\n",
    "            all_urls = f'https://www.largus.fr{all_urls}'\n",
    "\n",
    "        # Extraire les fiches techniques de la page \"Toutes les fiches techniques\"\n",
    "        fiches_techniques = extract_fiches_techniques(driver, all_urls, model)\n",
    "\n",
    "        if fiches_techniques:\n",
    "            # Sauvegarder les fiches techniques dans un fichier CSV\n",
    "            df_fiches_technique = pd.DataFrame(fiches_techniques)\n",
    "            folder_path = f\"Data/Fiches Techniques/{make.capitalize()}\"\n",
    "\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "                print(f\"Folder created at {folder_path}\")\n",
    "\n",
    "            file_name = f\"fiches_techniques_{model.lower()}.csv\"\n",
    "            save_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            df_fiches_technique.to_csv(save_path, index=False)\n",
    "            return df_fiches_technique\n",
    "        else:\n",
    "            print(\"Aucune fiche technique trouvée.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Lien vers Toutes les fiches techniques non trouvé.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_fiche_technique_file_links(driver, dataframe, column):\n",
    "    # Vérifier si la colonne Traiter existe déjà\n",
    "    if 'Traiter' not in dataframe.columns:\n",
    "        dataframe['Traiter'] = 0\n",
    "\n",
    "    filtered_df = dataframe[dataframe['Traiter'] == 1]\n",
    "    treated_links = set(filtered_df[column])  # Un ensemble pour stocker les liens déjà traités\n",
    "\n",
    "    counter = 0\n",
    "    for index, row in dataframe[len(treated_links):].iterrows():\n",
    "        link_url = row[column]\n",
    "        model = row['model']\n",
    "        make = row['make']\n",
    "        \n",
    "        # Vérifier si le lien a déjà été traité\n",
    "        if link_url in treated_links:\n",
    "            continue\n",
    "\n",
    "        if link_url not in treated_links:\n",
    "            driver.get(link_url)\n",
    "            html_content_fiche = driver.page_source\n",
    "            df_fiches_technique = process_all_fiches_techniques(html_content_fiche, model, make)\n",
    "            \n",
    "            if df_fiches_technique is None:\n",
    "                continue\n",
    "                \n",
    "            dataframe.at[index, 'Traiter'] = 1\n",
    "            treated_links.update(link_url)\n",
    "            save_file_path = f\"Data/Models/{make}.csv\"\n",
    "            save_file_path = unidecode.unidecode(save_file_path).strip().lower().replace(' ', '_').replace(\"'\", \"\")\n",
    "            dataframe.to_csv(save_file_path, index=False)\n",
    "            \n",
    "            counter += 1\n",
    "\n",
    "            print(f\"Waiting for 1 minute before the next URL...{counter}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        if counter >= 5:\n",
    "            print(f\"Arrêt après {counter} itérations.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Arrêt toutes les liens, un total de {counter} liens ont été traitées !.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire le lien vers \"Toutes les fiches techniques\"\n",
    "all_url = extract_all_fiches_techniques_url(html_content_fiche_technique)\n",
    "all_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiches_techniques = pd.read_csv('Fiches Techniques/Audi/fiches_techniques_fiche technique audi a3 berline 2024.csv')\n",
    "df_fiches_techniques.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### Fiche technique par lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Data/all_models.csv\"\n",
    "df_model = pd.read_csv(model_path)\n",
    "df_model[df_model['Traiter'] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_model[df_model['Traiter'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_model.to_csv('Data/all_models.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver()\n",
    "process_fiche_technique_file_links(driver, df_model, 'url')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concatenate_csvs(folder_path):\n",
    "    dataframes = []\n",
    "\n",
    "    # Parcourir tous les fichiers dans le dossier et les sous-dossiers\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                # Lire le fichier CSV et l'ajouter à la liste des DataFrames\n",
    "                _df = pd.read_csv(file_path)\n",
    "                dataframes.append(_df)\n",
    "\n",
    "    # Concaténer tous les DataFrames en un seul\n",
    "    final_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'Data/Fiches Techniques'\n",
    "df_fiches_techniques_final = load_and_concatenate_csvs(folder_path)\n",
    "df_fiches_techniques_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiches_techniques_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiches_techniques_final['Annee'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiches_techniques_final[df_fiches_techniques_final['Marque'] == 'Bmw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer toutes les occurrences de l'année 1007 par 2010 dans la colonne 'Annee'\n",
    "df_fiches_techniques_final.loc[df_fiches_techniques_final['Marque'] == 'Bmw', 'Traiter'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiches_techniques_final['Traiter'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mélanger les lignes du DataFrame\n",
    "df_fiches_techniques_final = df_fiches_techniques_final.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiches_techniques_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiches_techniques_final.to_csv('Data/Fiches Techniques/fiches_techniques_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "## Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_version = \"https://www.largus.fr/fiche-technique/Audi/A5/2024.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver()\n",
    "driver.get(url_version)\n",
    "# Attendre que la page se charge correctement (si nécessaire)\n",
    "driver.implicitly_wait(2)\n",
    "html_content = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_version_data(driver, url_version, html_content, df_rows):\n",
    "    \"\"\"\n",
    "    Extract version data from a given URL and HTML content using Selenium and BeautifulSoup.\n",
    "\n",
    "    Parameters:\n",
    "    driver (WebDriver): The Selenium WebDriver instance.\n",
    "    url_version (str): The URL containing the version information.\n",
    "    html_content (str): The HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lists containing version data.\n",
    "    str: The filename for the CSV file.\n",
    "    \"\"\"\n",
    "    # Extraire l'année de l'URL à l'aide d'une expression régulière\n",
    "    match = re.search(r'/(\\d{4})\\.html', url_version)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "    else:\n",
    "        year = datetime.now().year\n",
    "\n",
    "    # Localiser la table\n",
    "    table = driver.find_element(By.ID, 'listeVersions')\n",
    "    \n",
    "    if table is not None:\n",
    "        # Extraire les lignes de la table\n",
    "        rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "    \n",
    "        # Préparer une liste pour stocker les données\n",
    "        data_versions = []\n",
    "        mark = df_rows['Marque']\n",
    "        model = df_rows['Model']\n",
    "    \n",
    "        # Boucler à travers les lignes pour extraire les données\n",
    "        for row in rows[1:]:  # Ignorer l'en-tête\n",
    "            cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "            if cols:\n",
    "                version = cols[0].text\n",
    "                version_link = cols[0].find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                carrosserie = cols[1].text\n",
    "                energy = cols[2].text\n",
    "                boite = cols[3].text\n",
    "                puissance_fiscale = cols[4].text\n",
    "                data_versions.append([version, carrosserie, energy, boite, puissance_fiscale, version_link, year, mark, model])\n",
    "    \n",
    "        # Déterminer le nom du fichier CSV\n",
    "        if data_versions:\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "            title_tag = soup.find('h1', class_='title lvl1-title')\n",
    "            if title_tag:\n",
    "                title_text = title_tag.text.strip().lower()\n",
    "                title_text = re.sub(r'\\s+', '_', title_text)  # Remplacer les espaces par des underscores\n",
    "                csv_filename = f'{normalize_label(title_text)}.csv'\n",
    "            else:\n",
    "                csv_filename = f'fiches_techniques_{year}.csv'\n",
    "                \n",
    "            # Créer un DataFrame Pandas à partir des données\n",
    "            df_versions = pd.DataFrame(data_versions, columns=['Version', 'Carrosserie', 'Energie', 'Boîte', 'Puissance Fiscale', 'Url', 'Année', 'Marque', 'Modele'])\n",
    "            folder_path = f\"Versions/{mark}/{model}\"\n",
    "            \n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "            \n",
    "            save_path = os.path.join(folder_path, csv_filename)\n",
    "                \n",
    "            df_versions.to_csv(save_path, index=False)\n",
    "      \n",
    "    \n",
    "        return data_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_versions_links(driver, dataframe, column_link='Lien'):\n",
    "    # Vérifier si la colonne Traiter existe déjà\n",
    "    if 'Traiter' not in dataframe.columns:\n",
    "        dataframe['Traiter'] = 0\n",
    "\n",
    "    filtered_df = dataframe[dataframe['Traiter'] == 1]\n",
    "    treated_links = set(filtered_df[column_link])  # Un ensemble pour stocker les liens déjà traités\n",
    "\n",
    "    counter = 0\n",
    "    captcha = 0\n",
    "    for index, row in dataframe[len(treated_links):].iterrows():\n",
    "        link_url = row[column_link]\n",
    "\n",
    "        # Vérifier si le lien a déjà été traité\n",
    "        if link_url in treated_links:\n",
    "            continue\n",
    "\n",
    "        if link_url not in treated_links:\n",
    "            driver.get(link_url)\n",
    "            time.sleep(1)\n",
    "            html_content = driver.page_source\n",
    "            data_versions = extract_version_data(driver, link_url, html_content, row)\n",
    "\n",
    "            if data_versions is None:\n",
    "                captcha += 1\n",
    "                continue\n",
    "\n",
    "            dataframe.at[index, 'Traiter'] = 1\n",
    "            treated_links.update(link_url)\n",
    "            save_file_path = \"Fiches Techniques/fiches_techniques_final.csv\"\n",
    "            dataframe.to_csv(save_file_path, index=False)\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            print(f\"Waiting for 1 minute before the next URL...{counter}\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "        if captcha >= 2:\n",
    "            print(\"Detection de captcha\")\n",
    "            break\n",
    "            \n",
    "        if counter >= 50:\n",
    "            print(\"Arrêt après 50 itérations.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Arrêt toutes les liens, un total de {counter} liens ont été traitées !.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiches_techniques = pd.read_csv('Data/Fiches Techniques/fiches_techniques_final.csv')\n",
    "len(df_fiches_techniques[df_fiches_techniques['Traiter'] == 1]), len(df_fiches_techniques[df_fiches_techniques['Traiter'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_versions_links(driver, df_fiches_techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_version = pd.read_csv('Fiches Techniques/Audi/fiches_techniques_audi_a5_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_version.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "## Information Fiche technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fiche = 'https://www.largus.fr/fiche-technique/Bmw/X6/I+E71/2008/Break+5+Portes/30da+235+Exclusive-966560.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver()\n",
    "driver.get(url_fiche)\n",
    "# Attendre que la page se charge correctement (si nécessaire)\n",
    "driver.implicitly_wait(1)\n",
    "page_source = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "### Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vehicle_name(header):\n",
    "    vehicle_name_tag = header.find('span', class_='libelle-vehicule')\n",
    "    vehicle_name = vehicle_name_tag.text.strip() if vehicle_name_tag else None\n",
    "    return vehicle_name\n",
    "\n",
    "def extract_date_lancement(header):\n",
    "    date_lancement_tag = header.find('span', class_='date-lancement')\n",
    "    date_lancement = date_lancement_tag.text.strip() if date_lancement_tag else None\n",
    "    return date_lancement\n",
    "\n",
    "def extract_prix(header):\n",
    "    prix_tag = header.find('div', class_='prix')\n",
    "    prix = prix_tag.text.strip().replace('\\u00a0', ' ') if prix_tag else None\n",
    "    return prix\n",
    "\n",
    "def extract_gallery_images(soup, base_url=\"https://www.largus.fr\"):\n",
    "    gallery_div = soup.find('div', class_='galerieFT')\n",
    "    images = gallery_div.find_all('img') if gallery_div else []\n",
    "    image_urls = [base_url + img['src'] for img in images if 'src' in img.attrs]\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser le contenu de la page avec BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_header_data(soup):\n",
    "    \"\"\"\n",
    "    Extract vehicle information from the header section.\n",
    "\n",
    "    Parameters:\n",
    "    soup (BeautifulSoup): The BeautifulSoup object of the page.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the vehicle name, date of launch, and price.\n",
    "    \"\"\"\n",
    "    # Extraire les informations\n",
    "    header = soup.find('div', class_='title-bar clearfix')\n",
    "    vehicle = extract_vehicle_name(header)\n",
    "    date = extract_date_lancement(header)\n",
    "    price = extract_prix(header)\n",
    "\n",
    "\n",
    "    return [vehicle, price, date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_header = extract_header_data(soup)\n",
    "data_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_header = pd.DataFrame([data_header])\n",
    "df_data_header.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "### Gestion Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_images = extract_gallery_images(soup)\n",
    "gallery_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery = {\n",
    "    'Gallery Images' : gallery_images\n",
    "}\n",
    "df_gallery = pd.DataFrame([gallery])\n",
    "df_gallery.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "### Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(label):\n",
    "    return unidecode.unidecode(label).strip().replace(' ', '_').replace(\"'\", \"\")\n",
    "\n",
    "def extract_vehicle_resume(soup):\n",
    "    resume_div = soup.find('div', id='resume')\n",
    "\n",
    "    details = {}\n",
    "    # Extraire les informations détaillées\n",
    "    info_lines = resume_div.find_all('div', class_='ligneInfo')\n",
    "\n",
    "    for line in info_lines:\n",
    "        label = line.find('span', class_='labelInfo').text.strip().lower().replace(' ', '_')\n",
    "        value_element = line.find('span', class_='valeur')\n",
    "\n",
    "        if value_element:\n",
    "            value = ' '.join(value_element.text.split())\n",
    "        else:\n",
    "            value = '-'\n",
    "\n",
    "        details[normalize_label(label).upper()] = value\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_resume = extract_vehicle_resume(soup)\n",
    "vehicle_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = {\n",
    "    'Vehicule Resume': [vehicle_resume],\n",
    "}\n",
    "resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = pd.DataFrame(resume)\n",
    "df_resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "### Fonctions d'extraction par sous-titre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dimensions(soup):\n",
    "    dimensions = {}\n",
    "    dimensions_div = soup.find_all('div', class_='panel-dimPoids')\n",
    "    if dimensions_div:\n",
    "        for div in dimensions_div:\n",
    "            if div.find('h3', class_='sous-titre').text.strip().upper() == \"DIMENSIONS\":\n",
    "                dimension_lines = div.find_all('div', class_='ligneInfo')\n",
    "                for line_div in dimension_lines:\n",
    "                    label = line_div.find('span', class_='labelInfo').text.strip().lower().replace(' ', '_')\n",
    "                    value = ' '.join(line_div.find('span', class_='valeur').text.split())\n",
    "                    dimensions[normalize_label(label).upper()] = value\n",
    "        return dimensions\n",
    "\n",
    "def extract_weight(soup):\n",
    "    weights = {}\n",
    "    weight_divs = soup.find_all('div', class_='panel-dimPoids')\n",
    "    for div in weight_divs:\n",
    "        if div.find('h3', class_='sous-titre').text.strip().lower() == \"poids\":\n",
    "            weight_lines = div.find_all('div', class_='ligneInfo')\n",
    "            for line in weight_lines:\n",
    "                label = line.find('span', class_='labelInfo').text.strip().lower().replace(' ', '_')\n",
    "                value = ' '.join(line.find('span', class_='valeur').text.split())\n",
    "                weights[normalize_label(label).upper()] = value\n",
    "    return weights\n",
    "\n",
    "def extract_habitability(soup):\n",
    "    habitability = {}\n",
    "    habitability_divs = soup.find_all('div', class_='panel-dimPoids')\n",
    "    for div in habitability_divs:\n",
    "        if div.find('h3', class_='sous-titre').text.strip().lower() == \"habitabilité\":\n",
    "            habitability_lines = div.find_all('div', class_='ligneInfo')\n",
    "            for line in habitability_lines:\n",
    "                label = line.find('span', class_='labelInfo').text.strip().lower().replace(' ', '_')\n",
    "                value = ' '.join(line.find('span', class_='valeur').text.split())\n",
    "                habitability[normalize_label(label).upper()] = value\n",
    "    return habitability\n",
    "\n",
    "def extract_tires(soup):\n",
    "    tires = {}\n",
    "    tires_divs = soup.find_all('div', class_='panel-dimPoids')\n",
    "    for div in tires_divs:\n",
    "        if div.find('h3', class_='sous-titre').text.strip().lower() == \"pneumatiques\":\n",
    "            tires_lines = div.find_all('div', class_='ligneInfo')\n",
    "            for line in tires_lines:\n",
    "                label = line.find('span', class_='labelInfo').text.strip().lower().replace(' ', '_')\n",
    "                value = ' '.join(line.find('span', class_='valeur').text.split())\n",
    "                tires[normalize_label(label).upper()] = value\n",
    "    return tires\n",
    "\n",
    "def extract_vehicle_details(soup):\n",
    "    return [extract_dimensions(soup), extract_weight(soup), extract_habitability(soup), extract_tires(soup)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_details = extract_vehicle_details(soup)\n",
    "vehicle_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicle_details = pd.DataFrame(vehicle_details)\n",
    "df_vehicle_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "### Caractéristiques Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_engine_details(soup):\n",
    "    engine_details = {}\n",
    "    engine_div = soup.find('h3', class_='sous-titre', string='Moteur').find_next('div', class_='conteneur-infosFT')\n",
    "    if engine_div:\n",
    "        engine_lines = engine_div.find_all('div', class_='ligneInfo')\n",
    "        for line in engine_lines:\n",
    "            label = line.find('span', class_='labelInfo').text\n",
    "            value = ' '.join(line.find('span', class_='valeur').text.split())\n",
    "            engine_details[normalize_label(label).upper()] = value\n",
    "    return engine_details\n",
    "\n",
    "def extract_transmission_details(soup):\n",
    "    transmission_details = {}\n",
    "    transmission_div = soup.find('h3', class_='sous-titre', string='Transmission').find_next('div', class_='conteneur-infosFT')\n",
    "    if transmission_div:\n",
    "        transmission_lines = transmission_div.find_all('div', class_='ligneInfo')\n",
    "        for line in transmission_lines:\n",
    "            label = line.find('span', class_='labelInfo').text\n",
    "            value = ' '.join(line.find('span', class_='valeur').text.split())\n",
    "            transmission_details[normalize_label(label).upper()] = value\n",
    "    return transmission_details\n",
    "\n",
    "def extract_technical_details(soup):\n",
    "    technical_details = {}\n",
    "    technical_div = soup.find('h3', class_='sous-titre', string='Technique').find_next('div', class_='conteneur-infosFT')\n",
    "    if technical_div:\n",
    "        technical_lines = technical_div.find_all('div', class_='ligneInfo')\n",
    "        for line in technical_lines:\n",
    "            label = line.find('span', class_='labelInfo').text\n",
    "            value = ' '.join(line.find('span', class_='valeur').text.split())\n",
    "            technical_details[normalize_label(label).upper()] = value\n",
    "    return technical_details\n",
    "\n",
    "def extract_vehicle_characteristics(soup):\n",
    "    characteristics = {\n",
    "        'Engine': [extract_engine_details(soup)],\n",
    "        'Transmission': [extract_transmission_details(soup)],\n",
    "        'Technical': [extract_technical_details(soup)],\n",
    "    }\n",
    "    return characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_characteristics = extract_vehicle_characteristics(soup)\n",
    "vehicle_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicle_characteristics = pd.DataFrame(vehicle_characteristics)\n",
    "df_vehicle_characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "### Performances et les consommations du véhicule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_performance(soup):\n",
    "    performance_div = soup.find('div', class_='panel-heading', id='titre-pc')\n",
    "    if performance_div:\n",
    "        _div = performance_div.find_next_sibling('div', class_='panel-collapse').find('h3', string='Performances')\n",
    "        if _div:\n",
    "            performance_div = _div.find_next_sibling('div', class_='conteneur-infosFT')\n",
    "            if performance_div:\n",
    "                performance_data = {}\n",
    "                for info in performance_div.find_all('div', class_='ligneInfo'):\n",
    "                    label = info.find('span', class_='labelInfo').text.strip()\n",
    "                    value = info.find('span', class_='valeur').text.strip()\n",
    "                    performance_data[normalize_label(label).upper()] = value\n",
    "                return performance_data\n",
    "    return None\n",
    "\n",
    "def extract_consumption(soup):\n",
    "    consumption_div = soup.find('div', class_='panel-heading', id='titre-pc')\n",
    "    if consumption_div:\n",
    "        _div = consumption_div.find_next_sibling('div', class_='panel-collapse').find('h3', string='Consommations')\n",
    "        if _div:\n",
    "            consumption_div = _div.find_next('div', class_='conteneur-infosFT')\n",
    "            if consumption_div:\n",
    "                consumption_data = {}\n",
    "                for info in consumption_div.find_all('div', class_='ligneInfo'):\n",
    "                    label = info.find('span', class_='labelInfo').text.strip()\n",
    "                    value = info.find('span', class_='valeur').text.strip()\n",
    "                    consumption_data[normalize_label(label).upper()] = value\n",
    "                return consumption_data\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_data = extract_performance(soup)\n",
    "consumption_data = extract_consumption(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_and_consumption = {\n",
    "    'Performance': [performance_data],\n",
    "    'Consumption': [consumption_data]\n",
    "}\n",
    "\n",
    "performance_and_consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance_and_consumption = pd.DataFrame(performance_and_consumption)\n",
    "df_performance_and_consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(fiche_technical_detail):\n",
    "    df_fiche_technical_detail = pd.concat(fiche_technical_detail, ignore_index=False, axis=1)\n",
    "    return df_fiche_technical_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiche_technical_details = [\n",
    "    df_data_header,\n",
    "    df_resume,\n",
    "    df_vehicle_details,\n",
    "    df_vehicle_characteristics,\n",
    "    df_performance_and_consumption,\n",
    "    df_gallery\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiche_technical_details = combine_dataframes(fiche_technical_details)\n",
    "df_fiche_technical_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiche_technical_details.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_files_from_directory(root_dir):\n",
    "    all_dataframes = []\n",
    "\n",
    "    # Parcours du répertoire racine et de ses sous-répertoires\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    _df = pd.read_csv(file_path)\n",
    "                    all_dataframes.append(_df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors de la lecture de {file_path}: {e}\")\n",
    "\n",
    "    # Concaténer tous les DataFrames en un seul DataFrame\n",
    "    if all_dataframes:\n",
    "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    else:\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiche_technical_details.to_csv('Fiches Technical Details/fiches_technical_details.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_version = 'Versions/Bmw'\n",
    "df_versions = read_csv_files_from_directory(path_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_versions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_versions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_versions.to_csv(f\"{path_version}/{df_versions['Marque'][0]}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fiche_technical_details['Modele'] = pd.Series(dtype='str')\n",
    "# df_fiche_technical_details['Marque'] = pd.Series(dtype='str')\n",
    "# df_fiche_technical_details['Annee'] = pd.Series(dtype='str')\n",
    "# \n",
    "# for index, row in df_version_bmx.iterrows():\n",
    "#     model = row['Modele']\n",
    "#     mark = row['Marque']\n",
    "#     year = row['Année']\n",
    "# \n",
    "#     df_fiche_technical_details.loc[index, ['Modele', 'Marque', 'Annee']] = [model, mark, year]\n",
    "#     \n",
    "#     break\n",
    "# \n",
    "# df_fiche_technical_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiche_technical_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_immatriculation():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def process_vehicle_data(driver, save_file_path, column_link='Url'):\n",
    "    dataframe = pd.read_csv(save_file_path)\n",
    "   \n",
    "    # Vérifier si la colonne Traiter existe déjà\n",
    "    if 'Traiter' not in dataframe.columns:\n",
    "        dataframe['Traiter'] = 0\n",
    "\n",
    "    filtered_df = dataframe[dataframe['Traiter'] == 1]\n",
    "    treated_links = set(filtered_df[column_link])  # Un ensemble pour stocker les liens déjà traités\n",
    "\n",
    "    counter = 0\n",
    "    details = {\n",
    "        'Marque': [],\n",
    "        'Modele': [],\n",
    "        'Annee': [],\n",
    "        'Vehicule': [],\n",
    "        'Prix': [],\n",
    "        'Date Publication': [],\n",
    "        'Resumer': [],\n",
    "        'Dimensions': [],\n",
    "        'Weight': [],\n",
    "        'Habitability': [],\n",
    "        'Tires': [],\n",
    "        'Engine': [],\n",
    "        'Transmission': [],\n",
    "        'Technical': [],\n",
    "        'Performance': [],\n",
    "        'Consumption': [],\n",
    "        'Gallery Images': [],\n",
    "    }\n",
    "\n",
    "    for index, row in dataframe[len(treated_links):].iterrows():\n",
    "        link_url = row[column_link]\n",
    "\n",
    "        # Vérifier si le lien a déjà été traité\n",
    "        if link_url in treated_links:\n",
    "            continue\n",
    "\n",
    "        model = row['Modele']\n",
    "        mark = row['Marque']\n",
    "        year = row['Année']\n",
    "\n",
    "        \n",
    "        driver.get(link_url)\n",
    "        time.sleep(1)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        data_header = extract_header_data(soup)\n",
    "        vehicle_resume = extract_vehicle_resume(soup)\n",
    "        vehicle_details = extract_vehicle_details(soup)\n",
    "        vehicle_characteristics = extract_vehicle_characteristics(soup)\n",
    "        performance_data = extract_performance(soup)\n",
    "        consumption_data = extract_consumption(soup)\n",
    "        gallery_images = extract_gallery_images(soup)\n",
    "\n",
    "        details['Marque'].append(mark)\n",
    "        details['Modele'].append(model)\n",
    "        details['Annee'].append(year)\n",
    "        details['Vehicule'].append(data_header[0])\n",
    "        details['Prix'].append(data_header[1])\n",
    "        details['Date Publication'].append(data_header[2])\n",
    "        details['Resumer'].append(vehicle_resume)\n",
    "        details['Dimensions'].append(vehicle_details[0])\n",
    "        details['Weight'].append(vehicle_details[1])\n",
    "        details['Habitability'].append(vehicle_details[2])\n",
    "        details['Tires'].append(vehicle_details[3])\n",
    "        details['Engine'].append(vehicle_characteristics['Engine'])\n",
    "        details['Transmission'].append(vehicle_characteristics['Transmission'])\n",
    "        details['Technical'].append(vehicle_characteristics['Technical'])\n",
    "        details['Performance'].append(performance_data)\n",
    "        details['Consumption'].append(consumption_data)\n",
    "        details['Gallery Images'].append(gallery_images)\n",
    "\n",
    "        dataframe.at[index, 'Traiter'] = 1\n",
    "        treated_links.update(link_url)\n",
    "        counter += 1\n",
    "\n",
    "        print(f\"Waiting for 1 minute before the next URL...{counter}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        if counter >= 50:\n",
    "            print(f\"Arrêt après {counter} itérations.\")\n",
    "            break\n",
    "\n",
    "    dataframe.to_csv(save_file_path, index=False)\n",
    "\n",
    "    print(f\"Arrêt après {counter} itérations.\")\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_create_fiche_technical_df(data, folder):\n",
    "    columns = ['Resumer', 'Dimensions', 'Weight', 'Habitability', 'Tires', 'Engine',\n",
    "     'Transmission', 'Technical', 'Performance', 'Consumption']\n",
    "\n",
    "    # Vérifier si le fichier existe et charger les données existantes, sinon créer un DataFrame vide\n",
    "    if os.path.exists(folder):\n",
    "        try:\n",
    "            df_save = pd.read_csv(folder)\n",
    "        except EmptyDataError:\n",
    "            df_save = pd.DataFrame(columns=['Marque', 'Modele', 'Annee', 'Vehicule', 'Prix', 'Date Publication',\n",
    "                                            'Resumer', 'Dimensions', 'Weight', 'Habitability', 'Tires', 'Engine',\n",
    "                                            'Transmission', 'Technical', 'Performance', 'Consumption',\n",
    "                                            'Gallery Images'])\n",
    "    else:\n",
    "        df_save = pd.DataFrame(columns=['Marque', 'Modele', 'Annee', 'Vehicule', 'Prix', 'Date Publication',\n",
    "                                        'Resumer', 'Dimensions', 'Weight', 'Habitability', 'Tires', 'Engine',\n",
    "                                        'Transmission', 'Technical', 'Performance', 'Consumption',\n",
    "                                        'Gallery Images'])\n",
    "        \n",
    "    df_fiche = pd.DataFrame(data)\n",
    "\n",
    "    # Ajouter une colonne Immatriculation avec des valeurs uniques\n",
    "    df_fiche['Immatriculation'] = df_fiche.apply(lambda _: generate_immatriculation(), axis=1)\n",
    "\n",
    "    # Ajouter une colonne object_folder avec le chemin formaté\n",
    "    df_fiche['object_folder'] = df_fiche.apply(\n",
    "        lambda row: f\"Vehiculs/Version/{row['Marque'].capitalize()}/{row['Annee']}/{row['Vehicule'].lower()}\", axis=1\n",
    "    )\n",
    "    \n",
    "    for column in columns:\n",
    "        # Ajouter la clé Immatriculation dans chaque dictionnaire de colonne\n",
    "        df_fiche[column] = df_fiche.apply(\n",
    "            lambda row: {**row[column], 'Immatriculation': row['Immatriculation']} if isinstance(row[column], dict) else row[column], axis=1\n",
    "        )\n",
    "        # Ajouter la clé Object_Folder_{column} dans chaque dictionnaire de colonne\n",
    "        df_fiche[column] = df_fiche.apply(\n",
    "            lambda row: {**row[column], f\"Object_Folder_{column}\": f\"Vehiculs/Models/{row['Marque'].upper()}/{column}\"} if isinstance(row[column], dict) else row[column], axis=1\n",
    "        )\n",
    "\n",
    "    # Concaténer le DataFrame original avec le nouveau DataFrame\n",
    "    df_save = pd.concat([df_save, df_fiche], ignore_index=True)\n",
    "    # Enregistrer le DataFrame concaténé dans le fichier CSV\n",
    "    df_save.to_csv(folder, index=False)\n",
    "    \n",
    "    return df_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiche_technical_details = process_vehicle_data(driver, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_create_fiche_technical_df(fiche_technical_details, save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.largus import Largus, TechnicalDataSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_data_search = TechnicalDataSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"Data/Versions/Bmw/Bmw.csv\"\n",
    "df_versions = pd.read_csv(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_versions[df_versions['Traiter'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = technical_data_search.get_driver()\n",
    "data = technical_data_search.process_vehicle_data(driver, save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"Data/Versions/Bmw/Bmw.csv\"\n",
    "df = pd.read_csv(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Resumer'][0]['ENERGIE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save_folder = f\"Fiches Technical Details/{mark.capitalize()}\"\n",
    "# csv_file_path = f\"Fiches_Technical_Details_{mark.capitalize()}.csv\"\n",
    "\n",
    "#save_file_path = unidecode.unidecode(save_file_path).strip().replace(' ', '_').replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_directories(root_folder):\n",
    "    def read_subdirectories(folder):\n",
    "        dir_dict = {'name': os.path.basename(folder), 'subdirectories': []}\n",
    "        try:\n",
    "            for entry in os.scandir(folder):\n",
    "                if entry.is_dir(follow_symlinks=False):\n",
    "                    subdir = read_subdirectories(entry.path)\n",
    "                    dir_dict['subdirectories'].append(subdir)\n",
    "        except PermissionError:\n",
    "            pass\n",
    "        return dir_dict\n",
    "\n",
    "    return read_subdirectories(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'Data/Formats Type'  # Remplacez par le chemin de votre dossier racine\n",
    "directory_structure = read_directories(root_folder)\n",
    "directory_structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
